Media and Event Real Time Synchronization
=========================================

Media synchronization consists of two parts - event synchronization between devices and synchronization between timed media (usually video or audio) and other parts of an application (like annotations, subtitles).

Cross device real time event synchronization
--------------------------------------------

To allow timed synchronization between devices, the approximate communication delay between devices must be known. For this, the PZH needs to be able to send an event to each PZP and make a ‘best guess’ assumption for the one-way latency to the PZP, based on the turn-around time until a receipt is returned. In the simplest case, this will just be assumed to be half of the turn-around time, but event time stamps or other means can be used to achieve a better approximation. Alternately, other means of determining the latency (such as ‘ping’ messages) can be used.

Based on this information and a ‘SendSynced’ flag on an event, the PZH will send events out to subscribers in accordance to the (latest) known delay to achieve (as far as possible) a synchronized event for different subscribers.

E.g. three devices are subscribed to a synchronized event, the latest delays to the devices are 100 (PZP1), 250 (PZP2) and 500 (PZP3) milliseconds. An event reaching the PZH at t=0ms will be sent out to PZP3 immediately, to PZP2 at 250 ms and to PZP1 at 400 ms, to reach trigger activities at the same moment (t=500ms) on all devices.

### Description

The specification for events includes a ‘SendSynced’ flag. If this flag is set and the event has multiple destinations, the event distributor uses timing information (how this is obtained is outside the scope of the spec - a first approximation might be ‘ping’ times, but other methods can be used) to send the event to the recipients in a manner that their predicted arrival times are identical.

Media run time synchronization
------------------------------

Synchronization with audio or video media usually requires information about the current (absolute or relative) time position within a media stream. In most cases, this information can only be provided by the media renderer itself. To support synchronization with other media, webinos needs to define an interface allowing the media renderer to determine at what granularity timing events should be provided (every frame, every second, every ten seconds, only on start/stop/pause/end) and an interface allowing the renderer to create webinos events containing timing information. Depending on the specific media used, it might also be useful to allow other media based triggers and events to be sent by the media renderer to the webinos event handler, such as DSM-CC stream event objects or other DVB application signalling (DVB Blue Book A137).

